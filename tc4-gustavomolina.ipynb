{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tech Challenge 4 - Gustavo Molina Figueiredo RM 359124\n",
    "\n",
    "### Detecção facial, análise de expressões emocionais e detecção de atividades em vídeos\n",
    "\n",
    "Maiores explicações do código constam no relatório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando vídeo: 100%|██████████| 3326/3326 [09:12<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem final de movimentos: {'levantar_braco_direito': 2, 'levantar_braco_esquerdo': 3, 'levantar_mao(s)_na_altura_do_rosto': 10, 'levantar_mao(s)_na_altura_do_peito': 8, 'rosto_de_lado': 1, 'pessoa_sentada': 7, 'pessoa_em_pe': 4, 'braco_dobrado': 9}\n",
      "Contagem final de emoções: {'sad': 48, 'fear': 47, 'happy': 98, 'angry': 11, 'neutral': 79, 'surprise': 21, 'disgust': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path=\"pose_landmarker_full.task\")\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1,\n",
    "    min_pose_detection_confidence=0.9,\n",
    "    min_pose_presence_confidence=0.9,\n",
    "    min_tracking_confidence=0.9\n",
    ")\n",
    "\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "def is_right_arm_up(landmarks):\n",
    "    \"\"\"Verifica se o braço direito está levantado acima do ombro.\"\"\"\n",
    "    return landmarks[14].y < landmarks[12].y\n",
    "\n",
    "def is_left_arm_up(landmarks):\n",
    "    \"\"\"Verifica se o braço esquerdo está levantado acima do ombro.\"\"\"\n",
    "    return landmarks[13].y < landmarks[11].y\n",
    "\n",
    "def is_hands_on_face(landmarks):\n",
    "    \"\"\"Verifica se uma ou ambas as mãos estão na altura do rosto.\"\"\"\n",
    "    if any(l is None or l.y == 0.0 for l in [landmarks[0], landmarks[4], landmarks[9], landmarks[12], landmarks[19], landmarks[20]]):\n",
    "        return False\n",
    "    \n",
    "    shoulder_mouth_y = landmarks[12].y + ((landmarks[9].y - landmarks[12].y) / 2)\n",
    "    forehead_y = landmarks[0].y - (landmarks[4].y - landmarks[0].y)  # Estima a posição da testa\n",
    "    left_hand, right_hand = landmarks[19], landmarks[20]\n",
    "\n",
    "    return ((forehead_y < left_hand.y < shoulder_mouth_y) or (forehead_y < right_hand.y < shoulder_mouth_y))\n",
    "\n",
    "def is_hand_on_chest(landmarks):\n",
    "    \"\"\"Verifica se uma ou ambas as mãos estão na altura do peito.\"\"\"\n",
    "    required_landmarks = [11, 12, 19, 20, 23, 24]\n",
    "    if any(landmarks[i] is None for i in required_landmarks):\n",
    "        return False\n",
    "\n",
    "    shoulder_y1, shoulder_y2 = landmarks[11].y, landmarks[12].y  # Ombros\n",
    "    hip_y1, hip_y2 = landmarks[23].y, landmarks[24].y  # Quadris\n",
    "\n",
    "    shoulder_y = (shoulder_y1 + shoulder_y2) / 2  \n",
    "    hip_y = (hip_y1 + hip_y2) / 2  \n",
    "    upper_limit_y = shoulder_y + (hip_y - shoulder_y) / 6  \n",
    "    upper_torso_y = upper_limit_y + (hip_y - upper_limit_y) / 3  \n",
    "    shoulder_hip_alignment = abs(shoulder_y1 - shoulder_y2) < 0.07 and abs(hip_y1 - hip_y2) < 0.07 \n",
    "    left_hand, right_hand = landmarks[19], landmarks[20]\n",
    "\n",
    "    if shoulder_hip_alignment:\n",
    "        left_shoulder_x, right_shoulder_x = landmarks[11].x, landmarks[12].x\n",
    "        chest_x_min, chest_x_max = min(left_shoulder_x, right_shoulder_x), max(left_shoulder_x, right_shoulder_x)\n",
    "\n",
    "        return ((upper_limit_y <= left_hand.y < upper_torso_y and chest_x_min <= left_hand.x <= chest_x_max) or\n",
    "                (upper_limit_y <= right_hand.y < upper_torso_y and chest_x_min <= right_hand.x <= chest_x_max))\n",
    "    \n",
    "    else:\n",
    "        return ((upper_limit_y <= left_hand.y < upper_torso_y) or\n",
    "                (upper_limit_y <= right_hand.y < upper_torso_y))\n",
    "\n",
    "def is_face_sideways(landmarks):\n",
    "    \"\"\"Verifica se o rosto está virado de lado baseado na posição do nariz e olhos.\"\"\"\n",
    "    nose, left_eye, right_eye = landmarks[0], landmarks[2], landmarks[5]\n",
    "    \n",
    "    eyes_overlap = False\n",
    "    nose_misaligned = abs(nose.x - (left_eye.x + right_eye.x) / 2) > 0.04 \n",
    "\n",
    "    return eyes_overlap or nose_misaligned\n",
    "\n",
    "def get_valid_landmark(landmarks, index1, index2):\n",
    "    \"\"\"Retorna o primeiro ponto válido entre os dois índices.\"\"\"\n",
    "    if landmarks[index1].visibility > 0.5:\n",
    "        return landmarks[index1]\n",
    "    elif landmarks[index2].visibility > 0.5:\n",
    "        return landmarks[index2]\n",
    "    return None \n",
    "\n",
    "def is_sitting(landmarks):\n",
    "    \"\"\"Verifica se a pessoa está sentada comparando a altura do quadril e dos joelhos.\"\"\"\n",
    "    hip = get_valid_landmark(landmarks, 23, 24)  \n",
    "    knee = get_valid_landmark(landmarks, 25, 26)\n",
    "\n",
    "    if not hip or not knee:\n",
    "        return False  \n",
    "\n",
    "    return abs(hip.y - knee.y) < 0.2\n",
    "\n",
    "def is_standing(landmarks):\n",
    "    \"\"\"Verifica se a pessoa está de pé comparando a altura e o alinhamento lateral do quadril e joelhos.\"\"\"\n",
    "    hip = get_valid_landmark(landmarks, 23, 24) \n",
    "    knee = get_valid_landmark(landmarks, 25, 26)  \n",
    "\n",
    "    if not hip or not knee:\n",
    "        return False \n",
    "\n",
    "    hip_higher_than_knee = hip.y + 0.2 < knee.y\n",
    "\n",
    "    x_aligned = abs(hip.x - knee.x) < 0.2 if hip and knee else True\n",
    "\n",
    "    return hip_higher_than_knee and x_aligned\n",
    "\n",
    "def is_arm_bent(landmarks, margin=0.2):\n",
    "    \"\"\"Verifica se qualquer braço está dobrado com base na posição do ombro, cotovelo e pulso.\"\"\"\n",
    "    \n",
    "    right_shoulder = get_valid_landmark(landmarks, 11, 12)\n",
    "    right_elbow = get_valid_landmark(landmarks, 13, 14)\n",
    "    right_wrist = get_valid_landmark(landmarks, 15, 16)\n",
    "\n",
    "    def check_arm(shoulder, elbow, wrist):\n",
    "        \"\"\"Verifica se um braço específico está dobrado.\"\"\"\n",
    "        if not shoulder or not elbow or not wrist:\n",
    "            return False  \n",
    "\n",
    "        shoulder_elbow_vertical = abs(shoulder.x - elbow.x) < margin\n",
    "        elbow_wrist_horizontal = abs(elbow.y - wrist.y) < margin\n",
    "        condition_1 = shoulder_elbow_vertical and elbow_wrist_horizontal\n",
    "\n",
    "        shoulder_elbow_horizontal = abs(shoulder.y - elbow.y) < margin\n",
    "        elbow_wrist_vertical = abs(elbow.x - wrist.x) < margin\n",
    "        condition_2 = shoulder_elbow_horizontal and elbow_wrist_vertical\n",
    "\n",
    "        return condition_1 or condition_2\n",
    "\n",
    "    return check_arm(right_shoulder, right_elbow, right_wrist)\n",
    "\n",
    "last_emotions = {}\n",
    "\n",
    "emotion_history = {}\n",
    "\n",
    "def recognize_emotions(original_frame, emotions_count, history_limit=50):\n",
    "    \"\"\"Detecta emoções em um frame e conta apenas mudanças reais na emoção dominante.\"\"\"\n",
    "    global emotion_history\n",
    "    result = DeepFace.analyze(original_frame, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "    height, width, _ = original_frame.shape\n",
    "\n",
    "    for face in result:\n",
    "        x, y, w, h = face['region']['x'], face['region']['y'], face['region']['w'], face['region']['h']\n",
    "        dominant_emotion = face['dominant_emotion']\n",
    "\n",
    "        # Normaliza a posição do rosto para evitar pequenas variações no ID\n",
    "        face_id = (round(x / width, 2), round(y / height, 2))\n",
    "\n",
    "        if face_id not in emotion_history:\n",
    "            emotion_history[face_id] = deque(maxlen=history_limit)\n",
    "\n",
    "        # Verifica se a emoção já foi detectada recentemente\n",
    "        if len(emotion_history[face_id]) == 0 or emotion_history[face_id][-1] != dominant_emotion:\n",
    "            emotions_count[dominant_emotion] = emotions_count.get(dominant_emotion, 0) + 1\n",
    "            emotion_history[face_id].append(dominant_emotion)\n",
    "\n",
    "        # Desenha a detecção no frame\n",
    "        cv2.rectangle(original_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(original_frame, dominant_emotion, (x, y + h + 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "    return original_frame\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "def detect_anomalies(frame, prev_landmarks, threshold=0.35):\n",
    "    \"\"\"Detecta movimentos bruscos baseado na variação das coordenadas das articulações.\"\"\"\n",
    "    \n",
    "    global anomalies_count\n",
    "    \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        current_landmarks = np.array([[lm.x, lm.y] for lm in results.pose_landmarks.landmark])\n",
    "        \n",
    "        if prev_landmarks is not None:\n",
    "            movement_diff = np.linalg.norm(current_landmarks - prev_landmarks, axis=1).mean()\n",
    "            if movement_diff > threshold:\n",
    "                anomalies_count += 1\n",
    "        \n",
    "        return frame, current_landmarks\n",
    "    return frame, prev_landmarks\n",
    "\n",
    "anomalies_count = 0\n",
    "\n",
    "def detect_pose_and_count_movements(video_path, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Erro ao abrir o vídeo.\")\n",
    "        return\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    movements_count = {\n",
    "        \"levantar_braco_direito\": 0,\n",
    "        \"levantar_braco_esquerdo\": 0,\n",
    "        \"levantar_mao(s)_na_altura_do_rosto\": 0,\n",
    "        \"levantar_mao(s)_na_altura_do_peito\": 0,\n",
    "        \"rosto_de_lado\": 0,\n",
    "        \"pessoa_sentada\": 0,\n",
    "        \"pessoa_em_pe\": 0,\n",
    "        \"braco_dobrado\": 0\n",
    "    }\n",
    "\n",
    "    emotions_count = {}\n",
    "\n",
    "    previous_state = {key: False for key in movements_count.keys()}\n",
    "\n",
    "    prev_landmarks = None\n",
    "\n",
    "    for frame_index in tqdm(range(total_frames), desc=\"Processando vídeo\"):\n",
    "        ret, original_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "       \n",
    "        # Reconhecer emoções a cada n frames\n",
    "        if frame_index % 5 == 0:\n",
    "            frame_emotions = recognize_emotions(original_frame, emotions_count)\n",
    "            frame, prev_landmarks = detect_anomalies(frame_emotions, prev_landmarks)\n",
    "        else:\n",
    "            frame = original_frame\n",
    "       \n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "        detection_result = detector.detect_for_video(mp_image, frame_index)\n",
    "\n",
    "        if detection_result.pose_landmarks:\n",
    "            landmarks = detection_result.pose_landmarks[0]\n",
    "            states = {\n",
    "                \"levantar_braco_direito\": is_right_arm_up(landmarks),\n",
    "                \"levantar_braco_esquerdo\": is_left_arm_up(landmarks),\n",
    "                \"levantar_mao(s)_na_altura_do_rosto\": is_hands_on_face(landmarks),\n",
    "                \"levantar_mao(s)_na_altura_do_peito\": is_hand_on_chest(landmarks),\n",
    "                \"rosto_de_lado\": is_face_sideways(landmarks),\n",
    "                \"pessoa_sentada\": is_sitting(landmarks),\n",
    "                \"pessoa_em_pe\": is_standing(landmarks),\n",
    "                \"braco_dobrado\": is_arm_bent(landmarks)\n",
    "            }\n",
    "            \n",
    "            for key in states:\n",
    "                if states[key] and not previous_state[key]:\n",
    "                    movements_count[key] += 1\n",
    "                previous_state[key] = states[key]\n",
    "\n",
    "        # Exibir contagem de movimentos no vídeo\n",
    "        y_offset = 40\n",
    "        for key, value in movements_count.items():\n",
    "            cv2.putText(frame, f'{key.replace(\"_\", \" \").capitalize()}: {value}', (10, y_offset),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 0, 255), 1, cv2.LINE_AA)\n",
    "            y_offset += 20\n",
    "\n",
    "        # Exibir contagem de emoções no vídeo\n",
    "        for key, value in emotions_count.items():\n",
    "            cv2.putText(frame, f'{key.capitalize()}: {value}', (width - 200, y_offset),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "            y_offset += 20\n",
    "            \n",
    "        # Exibir contagem de frames processados\n",
    "        cv2.putText(frame, f'Frame: {frame_index + 1}/{total_frames}', (10, 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Exibir contagem de anomalias detectadas\n",
    "        cv2.putText(frame, f'Anomalias: {anomalies_count}', (10, y_offset),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        out.write(frame)\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(\"Contagem final de movimentos:\", movements_count)\n",
    "    print(\"Contagem final de emoções:\", emotions_count)\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "input_video_path = os.path.join(script_dir, 'video.mp4')\n",
    "output_video_path = os.path.join(script_dir, 'output_video.mp4')\n",
    "\n",
    "detect_pose_and_count_movements(input_video_path, output_video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
